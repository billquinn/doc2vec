{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Corpus Creation\n",
    "## Create Text Chunks in <i>The Masses</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file sorts and parses two different, though overlapping datasets, which are then used in creating a doc2vec model. The first dataset is structured data from the MJP, which includes genre information in each issue of publication. This data was previously parsed using the XML Parser in a separate repository. The second dataset is unstructured data from the <a href=\"https://repository.library.brown.edu/studio/collections/id_592/\">Modernist Journals Project in the Brown University Digital Repository</a>. \n",
    "\n",
    "This notebook provides three options for data exploration with these different corpora. The first option takes the structured data and chunks texts into strings (300-word strings at the moment). The resulting dataframe then joins with the unstructured, web-scrapped data. This produces some duplication but increases the number of words in the final, merged corpus and allows me to extract the text-chunks and their similarities to other text-chunks without losing key metadata. \n",
    "\n",
    "The second option similarly chunks texts into strings but does not merge with the structured data and, therefore, does not have as much bibliographic data.\n",
    "\n",
    "The third option uses the unstructured data from the web scrapper but does not chunk texts. This option can be useful for creating a corpus of issues (in the publication sense).\n",
    "\n",
    "This particular notebook extracts on text-chunks from the magazine, <i>The Masses</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re, string\n",
    "import nltk\n",
    "englishWords = set(nltk.corpus.words.words())\n",
    "\n",
    "abs_dir = \"/Users/williamquinn/Desktop/DH/Python/MJP/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Parse Structured Data of MJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 314 ms, total: 11.6 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load data.\n",
    "mjp_df = pd.read_csv(abs_dir + 'Output/mjp_documents.txt', sep='\\t')[['mjp_id', 'magazine', 'date', 'type', 'text']]\n",
    "\n",
    "# Subset to include key genres.\n",
    "# Script will only chunk selected genres and will re-combine with original dataframe.\n",
    "# The recombination should maintain model's accuracy, \n",
    "# which relies on an abundance of text,\n",
    "# while also modeling chunked data.\n",
    "mjp_df = mjp_df[mjp_df['type'] \\\n",
    "                .isin([\"articles\", \"letters\", \"fiction\", \n",
    "                       \"poetry\", \"drama\"])]\n",
    "\n",
    "# Remove bibliographic information (volume/issue, year) from strings.\n",
    "mjp_df['text'] = mjp_df['text'].astype(str) \\\n",
    "    .str.lower() \\\n",
    "    .str.strip() \\\n",
    "    .str.replace(r'[^\\w\\s]','', regex=True) \\\n",
    "    .str.replace(r\"pgbrk\",\"\", regex=True) \\\n",
    "    .str.replace('\\.0', '', regex=True) \\\n",
    "    .str.replace(r'vol \\w+ no \\d+ \\w+ \\d{4}', '', regex=True) \\\n",
    "    .str.replace(r'\\w+ \\d{4}', '', regex=True) \\\n",
    "    .str.replace(r'vol \\w+ no \\d+', '', regex=True) \\\n",
    "    .str.replace(r'v ', '', regex=True) \\\n",
    "    .str.replace(r'vol ', '', regex=True) \\\n",
    "    .str.replace(r'no ', '', regex=True) \\\n",
    "    .str.replace(r'the masses', '', regex=True) \\\n",
    "    .apply(lambda x: ' '.join([item for item in x.split() if item in englishWords]))\n",
    "\n",
    "mjp_df = mjp_df.rename(columns = {\"mjp_id\":\"mjp_index\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 (continued): <br>Incorporate Data with Chunked Texts from <i>The Masses</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.22 s, sys: 111 ms, total: 2.33 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# Subset dataframe for selected magazine.\n",
    "mjp_df_Masses = mjp_df.query('magazine == \"the masses\"')\n",
    "\n",
    "# Split text field string into list of 300 words.\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html\n",
    "# https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows\n",
    "# s = df['Seatblocks'].str.split(pat = \" \").apply(Series, 1).stack()\n",
    "\n",
    "# Split text field string into list of 300 words.\n",
    "def splitText(string):\n",
    "    words = string.split()\n",
    "    grouped_words = [' '.join(words[i: i + 300]) for i in range(0, len(words), 300)]\n",
    "    return grouped_words\n",
    "\n",
    "# This might cause errors.\n",
    "mjp_df_Masses['text'] = mjp_df_Masses['text'].apply(splitText)\n",
    "\n",
    "# Unnest list and create row for each list item.\n",
    "# https://mikulskibartosz.name/how-to-split-a-list-inside-a-dataframe-cell-into-rows-in-pandas-9849d8ff2401\n",
    "mjp_df_Masses = mjp_df_Masses['text'] \\\n",
    "    .apply(pd.Series) \\\n",
    "    .merge(mjp_df_Masses, right_index = True, left_index = True) \\\n",
    "    .drop([\"text\"], axis = 1) \\\n",
    "    .melt(id_vars = ['mjp_index', 'magazine', 'date', 'type'], value_name = \"text\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna()\n",
    "\n",
    "# Count words and remove short strings.\n",
    "mjp_df_Masses['count'] = mjp_df_Masses['text'].str.split().str.len()\n",
    "mjp_df_Masses = mjp_df_Masses[~(mjp_df_Masses['count'] < 300)]\n",
    "\n",
    "# Change index value to prevent duplication when merged with original dataframe.\n",
    "mjp_df_Masses['mjp_id'] = mjp_df_Masses.index + 10000\n",
    "\n",
    "mjp_df_Masses = mjp_df_Masses[[\"date\", \"magazine\", \"mjp_index\", \"text\", \"type\"]]\n",
    "\n",
    "# Rejoing dataframes\n",
    "mjp_concat_df = pd.concat([mjp_df, mjp_df_Masses], sort = True) \\\n",
    "    .dropna()\n",
    "\n",
    "mjp_concat_df.to_csv(abs_dir + 'Chapter4-Masses/Masses_Data/d2v/mjp_masses-chunks.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Parse MJP Scrapped from Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.82 s, sys: 953 ms, total: 6.77 s\n",
      "Wall time: 7.09 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mjp_df = pd.read_csv(abs_dir + \"Output/doc2vec/mjp_documents-EntireCorpus.csv\", \n",
    "                     sep=\",\") \\\n",
    "    .assign(type = \"issue\")\n",
    "\n",
    "# Split text field string into list of 300 words.\n",
    "def splitText(string):\n",
    "    words = string.split()\n",
    "    grouped_words = [' '.join(words[i: i + 300]) for i in range(0, len(words), 300)]\n",
    "    return grouped_words\n",
    "\n",
    "masses_df = mjp_df.query('magazine == \"Masses\"')\n",
    "\n",
    "# This is raising a warning and could be responsible for error during UMAP (unequal values).\n",
    "masses_df['text'] = masses_df['text'].apply(splitText)\n",
    "\n",
    "# Unnest list and create row for each list item.\n",
    "# https://mikulskibartosz.name/how-to-split-a-list-inside-a-dataframe-cell-into-rows-in-pandas-9849d8ff2401\n",
    "masses_df = masses_df['text'] \\\n",
    "    .apply(pd.Series) \\\n",
    "    .merge(masses_df, right_index = True, left_index = True) \\\n",
    "    .drop([\"text\"], axis = 1) \\\n",
    "    .melt(id_vars = ['mjp_index', 'magazine', 'date', 'type'], value_name = \"text\") \\\n",
    "    .drop(\"variable\", axis = 1) \\\n",
    "    .dropna()\n",
    "\n",
    "# Count words and remove short strings.\n",
    "masses_df['count'] = masses_df['text'].str.split().str.len()\n",
    "masses_df = masses_df[~(masses_df['count'] < 300)]\n",
    "\n",
    "masses_df['mjp_index'] = masses_df['mjp_index'] + 10000\n",
    "\n",
    "mjp_concat_df = pd.concat([mjp_df, masses_df], sort = True)\n",
    "\n",
    "mjp_concat_df \\\n",
    "    .dropna() \\\n",
    "    .to_csv(abs_dir + 'Chapter4-Masses/Masses_Data/d2v/mjp_masses-chunks-eC.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Merge Entire Corpus with Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 14.5 s, total: 34.1 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load Data\n",
    "mjp_df = pd.read_csv(abs_dir + \"Output/doc2vec/mjp_documents-EntireCorpus.csv\", \n",
    "                     sep=\",\") \\\n",
    "    .assign(type = \"issue\")\n",
    "\n",
    "subset_df = pd.read_csvmjp_df = pd.read_csv(abs_dir + 'Output/mjp_documents.txt', sep='\\t') \\\n",
    "    .query('magazine == \"the masses\"') \\\n",
    "    .rename(columns = {\"mjp_id\":\"mjp_index\"})[['mjp_index', 'magazine', 'date', 'type', 'text']]\n",
    "    \n",
    "# Regularize subset to match mjp_df.\n",
    "subset_df['magazine'] = \"Masses\"\n",
    "# Add values to disambiguate structured (>10000) and unstructured data.\n",
    "subset_df['mjp_index'] = subset_df['mjp_index'] + 10000\n",
    "\n",
    "# Append dataframes and remove empty text fields.\n",
    "mjp_appended_df = mjp_df.append(subset_df, ignore_index=True) \\\n",
    "    .dropna(subset=['text'])\n",
    "\n",
    "mjp_appended_df.to_csv(abs_dir + 'Chapter4-Masses/Masses_Data/d2v/mjp_masses-chunks-eC.csv', \n",
    "                       sep='\\t', \n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model on Masses Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 35s, sys: 2min 9s, total: 46min 45s\n",
      "Wall time: 20min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mjp_df = pd.read_csv(abs_dir + 'Chapter4-Masses/Masses_Data/d2v/mjp_masses-chunks-eC.csv', sep='\\t')\n",
    "\n",
    "# Create Corpus\n",
    "tagged_docs = mjp_df.apply \\\n",
    "    (lambda x:gensim.models.doc2vec.TaggedDocument \\\n",
    "     (gensim.utils.simple_preprocess(x.text), \\\n",
    "      ['doc{}'.format(x.mjp_index)]), axis=1)\n",
    "\n",
    "training_corpus = tagged_docs.values\n",
    "\n",
    "# Training.\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=4, epochs=20)\n",
    "\n",
    "model.build_vocab(training_corpus)\n",
    "\n",
    "model.train(training_corpus, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "# Store Model.\n",
    "model.save(abs_dir + \"Chapter4-Masses/Masses_Data/d2v/mjp_masses-chunks_d2v.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
